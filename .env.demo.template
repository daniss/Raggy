# Raggy Demo Environment Configuration
# Copy this file to .env and configure the values below

# === Environment ===
ENVIRONMENT=demo
DEBUG=true

# === CORS Configuration ===
CORS_ORIGINS=["http://localhost:3000","https://yourdomain.com"]

# === API Keys (REQUIRED) ===
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=deepseek-r1-distill-llama-70b

# === Supabase Configuration (REQUIRED) ===
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_service_key_here
DATABASE_URL=postgresql://postgres:password@localhost:5432/raggy

# === Demo Organization Configuration ===
DEMO_ORG_ID=demo-org-12345
MAX_DEMO_DOCUMENTS=100
DEMO_SESSION_EXPIRE_HOURS=24

# === RAG Configuration ===
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDINGS_PROVIDER=local
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_K=8

# === Remote Embeddings (when EMBEDDINGS_PROVIDER=remote) ===
# REMOTE_EMBEDDINGS_URL=https://api.openai.com/v1
# REMOTE_EMBEDDINGS_KEY=your_openai_api_key_here
# REMOTE_EMBEDDINGS_MODEL=text-embedding-3-small

# === LLM Provider Configuration ===
LLM_PROVIDER=groq
# LLM_BASE_URL=http://localhost:8000/v1  # For local LLM servers (vLLM/TGI)
# LOCAL_LLM_MODEL=default  # Model name for local LLM server
# LOCAL_LLM_API_KEY=your_local_llm_api_key  # Optional API key for local server

# === Performance Configuration ===
FAST_MODE=true
USE_HYBRID_SEARCH=true
USE_RERANKING=false
USE_QUERY_ENHANCEMENT=false
USE_SEMANTIC_CHUNKING=true
USE_ADAPTIVE_CHUNKING=true
MAX_CONTEXT_DOCS=5
ENABLE_LIGHTWEIGHT_RERANK=true

# === LLM Configuration ===
LLM_TEMPERATURE=0.0
MAX_TOKENS=3000

# === Cache Configuration (Optional) ===
REDIS_URL=redis://localhost:6379
CACHE_TTL_MINUTES=60
ENABLE_EMBEDDING_CACHE=true
ENABLE_QUERY_CACHE=true

# === Monitoring (Optional) ===
SENTRY_DSN=your_sentry_dsn_here

# === Client Branding (Optional) ===
CLIENT_NAME="Demo Company"
CLIENT_SLUG=demo-company