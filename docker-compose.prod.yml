version: '3.8'

services:
  # ==============================================
  # FRONTEND SERVICE - Next.js Application
  # ==============================================
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    container_name: raggy-frontend-prod
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL:-http://localhost:8000}
      - BACKEND_URL=http://backend:8000
      - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL}
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY}
      - NEXT_PUBLIC_APP_NAME=${NEXT_PUBLIC_APP_NAME:-Raggy - Assistant IA Privé}
      - NEXT_PUBLIC_APP_URL=${NEXT_PUBLIC_APP_URL}
      - NEXT_PUBLIC_ENVIRONMENT=production
      - NEXT_PUBLIC_ANALYTICS_ID=${NEXT_PUBLIC_ANALYTICS_ID}
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - raggy-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==============================================
  # BACKEND SERVICE - FastAPI Application
  # ==============================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: raggy-backend-prod
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    environment:
      # Application
      - ENVIRONMENT=production
      - DEBUG=false
      - API_TITLE=${API_TITLE:-Raggy - Assistant IA Privé}
      - API_DESCRIPTION=${API_DESCRIPTION:-Solution RAG sur-mesure pour entreprises}
      - API_VERSION=${API_VERSION:-2.0.0}
      
      # AI/LLM
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GROQ_MODEL=${GROQ_MODEL:-deepseek-r1-distill-llama-70b}
      - USE_LOCAL_LLM=${USE_LOCAL_LLM:-false}
      - LOCAL_LLM_URL=${LOCAL_LLM_URL:-http://local-llm:8080}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-dangvantuan/sentence-camembert-base}
      
      # Database
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      
      # Cache
      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}
      
      # Security
      - SECRET_KEY=${SECRET_KEY}
      - CORS_ORIGINS=${CORS_ORIGINS:-["http://localhost:3000"]}
      
      # Performance
      - MAX_DOCUMENTS=${MAX_DOCUMENTS:-1000}
      - MAX_UPLOAD_SIZE_MB=${MAX_UPLOAD_SIZE_MB:-50}
      - MAX_CONTEXT_DOCS=${MAX_CONTEXT_DOCS:-5}
      - FAST_MODE=${FAST_MODE:-true}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-true}
      
      # Monitoring
      - SENTRY_DSN=${SENTRY_DSN}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - raggy-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==============================================
  # DATABASE SERVICE - PostgreSQL 16 + pgvector
  # ==============================================
  db:
    image: pgvector/pgvector:pg16
    container_name: raggy-database-prod
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${DB_NAME:-raggy_db}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-raggy_secure_password_2024!}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C.UTF-8"
      POSTGRES_HOST_AUTH_METHOD: scram-sha-256
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
      - ./backend/database_schema_simple.sql:/docker-entrypoint-initdb.d/01-schema.sql
    ports:
      - "${DB_PORT:-5432}:5432"
    command: |
      postgres 
      -c shared_preload_libraries=vector
      -c max_connections=200
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=8MB
    networks:
      - raggy-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d ${DB_NAME:-raggy_db}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ==============================================
  # REDIS SERVICE - Cache and Job Queue
  # ==============================================
  redis:
    image: redis:7-alpine
    container_name: raggy-redis-prod
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru --save 60 1000
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - raggy-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==============================================
  # LOCAL LLM SERVICE (OPTIONAL) - vLLM/TGI
  # ==============================================
  # Uncomment to enable local LLM inference
  # local-llm:
  #   image: vllm/vllm-openai:latest
  #   container_name: raggy-local-llm
  #   restart: unless-stopped
  #   ports:
  #     - "${VLLM_PORT:-8080}:8000"
  #   environment:
  #     - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
  #     - VLLM_PORT=8000
  #     - VLLM_MODEL=${VLLM_MODEL_PATH:-microsoft/DialoGPT-medium}
  #     - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.8}
  #     - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-4096}
  #   volumes:
  #     - ./models:/models
  #     - ./cache:/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   networks:
  #     - raggy-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 60s
  #     timeout: 30s
  #     retries: 3
  #     start_period: 300s

  # ==============================================
  # NGINX REVERSE PROXY (PRODUCTION)
  # ==============================================
  nginx:
    image: nginx:alpine
    container_name: raggy-nginx-prod
    restart: unless-stopped
    ports:
      - "${NGINX_PORT:-80}:80"
      - "${NGINX_SSL_PORT:-443}:443"
    environment:
      - DOMAIN_NAME=${DOMAIN_NAME:-localhost}
      - API_DOMAIN=${API_DOMAIN:-api.localhost}
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
      - ./public:/var/www/public:ro
    depends_on:
      frontend:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - raggy-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local

networks:
  raggy-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16